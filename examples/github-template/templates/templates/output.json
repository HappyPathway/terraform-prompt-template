{
  "readme_content": "\"\"\"\n# smart-invest\n**Organization:** HappyPathway\n\n## 1. Overview\n\n`smart-invest` is a Python-based application designed for automated analysis of trade and investment data. It leverages publicly available APIs and databases to gather financial information, employs AI (specifically Google's Gemini model via `instructor` and Google Search) for data interpretation and insight generation, predicts market patterns, and generates daily PDF reports. These reports, which include a self-assessment of past prediction accuracy, are then emailed to subscribers. The entire process is orchestrated to run as a cron job using GitHub Actions, with data persistence managed via SQLite and Google Cloud Storage (GCS).\n\n## 2. Features\n\n- **Automated Data Ingestion**: Fetches financial data from various public APIs and databases.\n- **Data Analysis**: Performs analysis on trade and investment data.\n- **AI-Powered Insights**: Uses Pydantic with `instructor` to query Google's Gemini model and Google Search for data enrichment, analysis, and generating textual summaries or insights.\n- **Pattern Prediction**: Implements models to predict potential market patterns. (Initial scope may involve simpler models, with capability to expand).\n- **Prediction Tracking & Self-Grading**: Stores historical investment predictions and their outcomes, automatically grading its prediction accuracy over time.\n- **Dynamic PDF Reporting**: Generates daily PDF reports summarizing analysis, predictions, AI-generated insights, and historical prediction accuracy.\n- **Email Notifications**: Distributes the generated PDF reports to a list of subscribers.\n- **Scheduled Execution**: Runs automatically on a defined schedule (e.g., daily) using GitHub Actions cron jobs.\n- **Persistent Storage**: Utilizes SQLite for local data storage (predictions, accuracy history, configurations), with the database file synchronized with Google Cloud Storage (GCS) for persistence across job runs.\n- **Modular Architecture**: Designed with multiple components (agents/models) working in concert for different tasks like data fetching, analysis, LLM interaction, prediction, and reporting.\n\n## 3. Architecture\n\nThe system is designed as a series of interconnected modules orchestrated by a main application script, which is triggered by GitHub Actions:\n\n1.  **Orchestrator (GitHub Actions)**:\n    *   Runs on a cron schedule.\n    *   Sets up the Python environment and installs dependencies.\n    *   **GCS Sync (Pull)**: Downloads the latest SQLite database from GCS.\n    *   Executes the main application logic.\n    *   **GCS Sync (Push)**: Uploads the updated SQLite database to GCS.\n\n2.  **Data Ingestion Module**:\n    *   Fetches raw data from configured public financial APIs (e.g., stock prices, economic indicators, news feeds).\n    *   Uses `httpx` for efficient HTTP requests.\n    *   Validates incoming data using Pydantic models.\n\n3.  **LLM Integration Module (`instructor` with Gemini & Google Search)**:\n    *   Uses `instructor` to get Pydantic-validated structured output from Google's Gemini model.\n    *   Queries Gemini for data analysis, summarization, sentiment analysis, or generating narrative insights based on fetched data.\n    *   Utilizes Google Search API to find relevant recent news, articles, or specific data points to augment analysis.\n    *   Pydantic models define the expected schema for LLM responses.\n\n4.  **Data Storage Module (SQLAlchemy & SQLite)**:\n    *   Defines database schema using SQLAlchemy ORM.\n    *   Stores fetched data, analysis results, prediction parameters, historical predictions, actual outcomes, and accuracy scores in a SQLite database.\n    *   Alembic can be used for schema migrations.\n\n5.  **Analysis & Prediction Engine**:\n    *   **Analysis Core**: Processes and analyzes the fetched data.\n    *   **Pattern Prediction Models**: Implements one or more algorithms to predict future patterns or trends. This can range from statistical methods to machine learning models.\n    *   Predictions are stored along with the rationale or key factors, potentially derived from LLM insights.\n\n6.  **Self-Assessment Module (Historian)**:\n    *   Retrieves past predictions from the database.\n    *   Compares them with actual market outcomes (fetched or manually updated).\n    *   Calculates accuracy scores (e.g., MAPE, directional accuracy).\n    *   Updates the database with these scores.\n    *   This feedback loop is crucial for iterative improvement and transparent reporting.\n\n7.  **Reporting Engine (PDF Generation)**:\n    *   Uses a library like WeasyPrint to convert HTML/CSS templates into PDF reports.\n    *   Templates are populated with data including: market summary, new predictions, AI-generated insights, historical prediction performance (including accuracy grade).\n\n8.  **Notification Module (Email)**:\n    *   Sends the generated PDF reports to a predefined list of subscribers.\n    *   Uses `smtplib` for sending emails.\n\n## 4. Tech Stack\n\n- **Programming Language**: Python (3.10+ recommended, ideally 3.11 or 3.12)\n- **Data Validation & Settings**: Pydantic (v2)\n- **LLM Interaction**: `instructor` (for Pydantic + LLMs), `google-generativeai` (for Gemini API), `google-api-python-client` (for Google Search API)\n- **Database**: SQLite\n- **ORM**: SQLAlchemy (v2.x)\n- **Database Migrations**: Alembic (optional, but recommended for schema evolution)\n- **Cloud Storage**: Google Cloud Storage (GCS) via `google-cloud-storage` library\n- **HTTP Client**: `httpx` (preferred for async capabilities) or `requests`\n- **PDF Generation**: WeasyPrint (recommended), ReportLab, or FPDF\n- **Email**: `smtplib` (Python standard library)\n- **Dependency Management**: Poetry\n- **Linting/Formatting**: Ruff\n- **Testing**: Pytest\n- **CI/CD & Scheduling**: GitHub Actions\n- **Potential ML Libraries (for advanced prediction)**: Scikit-learn, Statsmodels, Prophet, TensorFlow/Keras, PyTorch\n\n## 5. Prerequisites\n\n- Python (see Tech Stack for version)\n- A Google Cloud Platform (GCP) account with:\n    - A GCS bucket created.\n    - Service account credentials (JSON keyfile) with permissions to read/write to the GCS bucket.\n- API Keys for:\n    - Google AI Studio (for Gemini API).\n    - Google Cloud Console (for Google Search API, if a custom search engine is used, or other Google APIs).\n    - Any financial data APIs being used.\n- Email account credentials (SMTP server, port, username, password) for sending reports.\n\n## 6. Project Setup\n\n1.  **Clone the Repository**:\n    ```bash\n    git clone https://github.com/HappyPathway/smart-invest.git\n    cd smart-invest\n    ```\n\n2.  **Install Python**: Ensure you have a compatible Python version installed. (e.g., Python 3.11 from `https://www.python.org/downloads/`)\n\n3.  **Setup Poetry**: If not already installed, install Poetry:\n    ```bash\n    curl -sSL https://install.python-poetry.org | python3 -\n    ```\n    (Follow official Poetry installation instructions for your OS: `https://python-poetry.org/docs/#installation`)\n\n4.  **Install Dependencies**: Poetry will create a virtual environment and install dependencies from `pyproject.toml` and `poetry.lock`.\n    ```bash\n    poetry install\n    ```\n\n5.  **Configuration (`.env` file)**:\n    Create a `.env` file in the project root. This file is ignored by Git (`.gitignore` should list `.env`). Populate it with necessary credentials and configurations. Pydantic's `BaseSettings` will load these.\n\n    Example `.env` file:\n    ```env\n    # GCP Configuration\n    GCP_PROJECT_ID=\"your-gcp-project-id\"\n    GCS_BUCKET_NAME=\"your-smart-invest-gcs-bucket\"\n    GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/gcp-service-account-key.json\"\n\n    # Gemini API Key\n    GEMINI_API_KEY=\"your_gemini_api_key\"\n\n    # Google Search API (example, specific keys depend on setup)\n    GOOGLE_SEARCH_API_KEY=\"your_google_search_api_key\"\n    GOOGLE_SEARCH_CX=\"your_custom_search_engine_id\"\n\n    # Financial APIs (example)\n    ALPHA_VANTAGE_API_KEY=\"your_alpha_vantage_key\"\n\n    # Email Configuration\n    SMTP_HOST=\"smtp.example.com\"\n    SMTP_PORT=587\n    SMTP_USERNAME=\"your_email@example.com\"\n    SMTP_PASSWORD=\"your_email_password\"\n    EMAIL_SENDER=\"smart-invest@example.com\"\n    EMAIL_SUBSCRIBERS=\"subscriber1@example.com,subscriber2@example.com\"\n\n    # Application Settings\n    DATABASE_URL=\"sqlite:///./smart_invest.db\" # Path to the local SQLite file\n    LOG_LEVEL=\"INFO\"\n    ```\n\n6.  **GCS Setup**:\n    *   Create a GCS bucket in your GCP project.\n    *   Download the JSON service account key and update `GOOGLE_APPLICATION_CREDENTIALS` in your `.env` file or set it as an environment variable in GitHub Actions secrets.\n\n7.  **Database Initialization (if using Alembic)**:\n    If you're using Alembic for migrations:\n    ```bash\n    poetry run alembic upgrade head\n    ```\n    Otherwise, SQLAlchemy will create the tables based on model definitions on first run if designed to do so.\n\n## 7. Configuration Details\n\nApplication configuration is managed via Pydantic's `BaseSettings` class, which loads values from environment variables (and the `.env` file for local development).\n\nKey configuration variables (refer to `.env` example):\n- `GCP_PROJECT_ID`, `GCS_BUCKET_NAME`, `GOOGLE_APPLICATION_CREDENTIALS`\n- `GEMINI_API_KEY`\n- `GOOGLE_SEARCH_API_KEY`, `GOOGLE_SEARCH_CX` (if applicable)\n- Financial API keys (e.g., `ALPHA_VANTAGE_API_KEY`)\n- `SMTP_HOST`, `SMTP_PORT`, `SMTP_USERNAME`, `SMTP_PASSWORD`\n- `EMAIL_SENDER`, `EMAIL_SUBSCRIBERS` (comma-separated list)\n- `DATABASE_URL` (e.g., `sqlite:///./data/smart_invest.db`)\n- `LOG_LEVEL` (e.g., `INFO`, `DEBUG`)\n\nThese will be defined in a `config.py` module using Pydantic.\n\n## 8. Usage\n\n### Local Execution\n\nTo run the application locally (e.g., for testing a full cycle):\n\n```bash\npoetry run python main.py\n```\n(Assuming `main.py` is your main application script.)\n\n### GitHub Actions Workflow\n\nThe application is designed to run automatically via a GitHub Actions workflow defined in `.github/workflows/daily_run.yml`.\n\nKey aspects of the workflow:\n- **Trigger**: Cron schedule (e.g., `cron: '0 8 * * *'` for daily at 8 AM UTC).\n- **Secrets**: All API keys, GCS credentials, and email passwords must be stored as GitHub Encrypted Secrets (e.g., `GCP_SA_KEY_B64` (base64 encoded JSON), `GEMINI_API_KEY`, `SMTP_PASSWORD`).\n- **Steps**:\n    1.  Checkout repository.\n    2.  Set up Python.\n    3.  Install Poetry and dependencies (`poetry install --no-root --no-dev`).\n    4.  Authenticate to Google Cloud (e.g., using `google-github-actions/auth`).\n    5.  Download SQLite DB from GCS.\n    6.  Run the main application script (`poetry run python main.py`).\n    7.  Upload updated SQLite DB to GCS.\n\n## 9. Modules Overview (Conceptual)\n\n- **`smart_invest/`**\n    - **`main.py`**: Main application script, orchestrates the workflow.\n    - **`config.py`**: Pydantic settings management.\n    - **`core/`**: Core logic and shared utilities.\n        - **`logger.py`**: Logging setup.\n    - **`data_sources/`**: Modules for fetching data from various APIs.\n        - `public_api_client.py`\n    - **`gcs/`**: GCS interaction logic.\n        - `client.py` (upload/download DB)\n    - **`database/`**: SQLAlchemy models, session management, CRUD operations.\n        - `models.py`\n        - `crud.py`\n        - `session.py`\n    - **`llm/`**: Integration with Gemini and Google Search via `instructor`.\n        - `client.py` (Gemini client setup)\n        - `prompts.py`\n        - `schemas.py` (Pydantic models for LLM output)\n        - `services.py` (functions to query LLMs)\n    - **`analysis/`**: Data analysis logic.\n        - `analyzer.py`\n    - **`prediction/`**: Prediction models and logic.\n        - `models/` (different prediction model implementations)\n        - `predictor.py`\n    - **`reporting/`**: PDF report generation.\n        - `generator.py`\n        - `templates/` (HTML/CSS templates for PDF)\n    - **`notifications/`**: Emailing service.\n        - `emailer.py`\n    - **`history/`**: Prediction tracking and self-grading logic.\n        - `assessor.py`\n- **`tests/`**: Pytest tests for all modules.\n- **`scripts/`**: Utility scripts (e.g., one-off tasks, backfills).\n- **`alembic/`**: (If using Alembic) migration scripts.\n- **`.github/workflows/`**: GitHub Actions workflows.\n    - `daily_run.yml`\n- **`pyproject.toml`**: Project metadata and dependencies for Poetry.\n- **`.env.example`**: Template for environment variables.\n\n## 10. Development\n\n- **Linting & Formatting**: Uses Ruff. Configure in `pyproject.toml` or `ruff.toml`.\n  ```bash\n  poetry run ruff check . --fix\n  poetry run ruff format .\n  ```\n- **Testing**: Uses Pytest. Tests are located in the `tests/` directory.\n  ```bash\n  poetry run pytest\n  ```\n- **Pre-commit Hooks**: Consider setting up pre-commit hooks to automate linting/formatting before commits.\n\n## 11. Error Handling and Logging\n\n- Implement robust error handling, especially for network requests, API interactions, and data processing.\n- Use structured logging (e.g., JSON format) to make logs easier to parse and analyze, especially in GitHub Actions.\n\n## 12. Security Considerations\n\n- **Never commit secrets** (API keys, passwords, GCP service account keys) to the repository.\n- Use GitHub Encrypted Secrets for CI/CD pipelines.\n- For local development, use `.env` files and add `.env` to `.gitignore`.\n- Regularly review GCP IAM permissions for the service account.\n- Be mindful of rate limits for all external APIs.\n\n## 13. Future Enhancements\n\n- More sophisticated prediction models (e.g., ML-based).\n- Interactive web dashboard for viewing reports and historical data.\n- User authentication for accessing reports or managing subscriptions.\n- More data sources and types of analysis.\n- Advanced anomaly detection.\n- Backtesting framework for prediction models.\n\n---\nThis document provides a comprehensive guide for the `smart-invest` project. For specific technical details, refer to the linked documentation for each tool and library.\n\"\"\"\n",
  "best_practices": [
    "Modular Design: Structure the application into loosely coupled modules (data fetching, analysis, prediction, reporting, LLM interaction, etc.) for better maintainability, testability, and scalability. Each 'agent' or 'model' can be a distinct module.",
    "Dependency Management: Utilize Poetry for robust dependency management, ensuring reproducible builds and a clear project structure. Commit the `poetry.lock` file.",
    "Configuration Management: Externalize all configurations (API keys, GCS bucket details, email settings, model parameters) using environment variables. Leverage Pydantic's `BaseSettings` for loading, validating, and type-hinting configurations.",
    "Data Validation with Pydantic: Employ Pydantic extensively for validating incoming data from external APIs, database integrity, configuration values, and ensuring structured, typed outputs from LLM interactions (using `instructor` with Gemini).",
    "Automated Testing: Implement comprehensive unit and integration tests using Pytest. Focus on testing data processing pipelines, prediction logic, LLM interactions, report generation, and the self-grading mechanism.",
    "CI/CD with GitHub Actions: Automate linting (Ruff), testing, and the daily cron job execution using GitHub Actions. Securely manage all secrets (API keys, GCS credentials) using GitHub Encrypted Secrets.",
    "Robust Logging: Implement structured logging throughout the application (e.g., using the standard `logging` module configured for JSON output) to facilitate debugging and monitoring of the cron jobs.",
    "Idempotent GCS Synchronization: Ensure that the process of pulling the SQLite database from GCS and pushing it back is idempotent and handles potential race conditions or failures gracefully, especially in a cron job environment.",
    "SQLAlchemy and Alembic: Use SQLAlchemy 2.0 for ORM interactions with the SQLite database. Implement Alembic for database schema migrations to manage changes to the database structure over time.",
    "Ethical AI and Data Usage: When using Gemini and Google Search, be mindful of rate limits, terms of service, and the ethical implications of interpreting and reporting AI-generated information, especially for financial predictions."
  ],
  "suggested_extensions": [
    "ms-python.python",
    "ms-python.vscode-pylance",
    "charliermarsh.ruff",
    "njpwerner.autodocstring",
    "eamodio.gitlens",
    "GitHub.copilot",
    "redhat.vscode-yaml",
    "esbenp.prettier-vscode"
  ],
  "documentation_source": [
    "Python Official Documentation: https://docs.python.org/3/",
    "Pydantic: https://docs.pydantic.dev/latest/",
    "Instructor (Pydantic with LLMs): https://python.useinstructor.com/",
    "Google AI Python SDK (Gemini): https://ai.google.dev/docs/sdk / https://github.com/google/generative-ai-python",
    "Google API Python Client (for Google Search, etc.): https://github.com/googleapis/google-api-python-client",
    "Google Cloud Storage Python Client: https://cloud.google.com/python/docs/reference/storage/latest",
    "SQLAlchemy (Core and ORM): https://docs.sqlalchemy.org/en/20/",
    "Alembic (Database Migrations for SQLAlchemy): https://alembic.sqlalchemy.org/en/latest/",
    "WeasyPrint (HTML/CSS to PDF): https://weasyprint.readthedocs.io/en/stable/",
    "ReportLab (PDF Generation): https://www.reportlab.com/docs/reportlab-userguide.pdf",
    "HTTPLib (`httpx` - async HTTP client): https://www.python-httpx.org/",
    "Requests (HTTP client): https://requests.readthedocs.io/en/latest/",
    "GitHub Actions: https://docs.github.com/en/actions",
    "Ruff (Linter & Formatter): https://docs.astral.sh/ruff/",
    "Poetry (Dependency Management): https://python-poetry.org/docs/",
    "Pytest (Testing Framework): https://docs.pytest.org/en/stable/",
    "Standard Library `smtplib` (Email): https://docs.python.org/3/library/smtplib.html",
    "Standard Library `email.mime`: https://docs.python.org/3/library/email.mime.html"
  ],
  "copilot_instructions": "# GitHub Copilot Instructions for smart-invest\n\n## Project Context\n\n\n\n## Best Practices\n\n\n- Modular Design: Structure the application into loosely coupled modules (data fetching, analysis, prediction, reporting, LLM interaction, etc.) for better maintainability, testability, and scalability. Each 'agent' or 'model' can be a distinct module.\n\n- Dependency Management: Utilize Poetry for robust dependency management, ensuring reproducible builds and a clear project structure. Commit the `poetry.lock` file.\n\n- Configuration Management: Externalize all configurations (API keys, GCS bucket details, email settings, model parameters) using environment variables. Leverage Pydantic's `BaseSettings` for loading, validating, and type-hinting configurations.\n\n- Data Validation with Pydantic: Employ Pydantic extensively for validating incoming data from external APIs, database integrity, configuration values, and ensuring structured, typed outputs from LLM interactions (using `instructor` with Gemini).\n\n- Automated Testing: Implement comprehensive unit and integration tests using Pytest. Focus on testing data processing pipelines, prediction logic, LLM interactions, report generation, and the self-grading mechanism.\n\n- CI/CD with GitHub Actions: Automate linting (Ruff), testing, and the daily cron job execution using GitHub Actions. Securely manage all secrets (API keys, GCS credentials) using GitHub Encrypted Secrets.\n\n- Robust Logging: Implement structured logging throughout the application (e.g., using the standard `logging` module configured for JSON output) to facilitate debugging and monitoring of the cron jobs.\n\n- Idempotent GCS Synchronization: Ensure that the process of pulling the SQLite database from GCS and pushing it back is idempotent and handles potential race conditions or failures gracefully, especially in a cron job environment.\n\n- SQLAlchemy and Alembic: Use SQLAlchemy 2.0 for ORM interactions with the SQLite database. Implement Alembic for database schema migrations to manage changes to the database structure over time.\n\n- Ethical AI and Data Usage: When using Gemini and Google Search, be mindful of rate limits, terms of service, and the ethical implications of interpreting and reporting AI-generated information, especially for financial predictions.\n\n\n\n\n## Recommended VS Code Extensions\n\n\n- ms-python.python\n\n- ms-python.vscode-pylance\n\n- charliermarsh.ruff\n\n- njpwerner.autodocstring\n\n- eamodio.gitlens\n\n- GitHub.copilot\n\n- redhat.vscode-yaml\n\n- esbenp.prettier-vscode\n\n\n\n\n## Documentation Sources\n\n\n- Python Official Documentation: https://docs.python.org/3/\n\n- Pydantic: https://docs.pydantic.dev/latest/\n\n- Instructor (Pydantic with LLMs): https://python.useinstructor.com/\n\n- Google AI Python SDK (Gemini): https://ai.google.dev/docs/sdk / https://github.com/google/generative-ai-python\n\n- Google API Python Client (for Google Search, etc.): https://github.com/googleapis/google-api-python-client\n\n- Google Cloud Storage Python Client: https://cloud.google.com/python/docs/reference/storage/latest\n\n- SQLAlchemy (Core and ORM): https://docs.sqlalchemy.org/en/20/\n\n- Alembic (Database Migrations for SQLAlchemy): https://alembic.sqlalchemy.org/en/latest/\n\n- WeasyPrint (HTML/CSS to PDF): https://weasyprint.readthedocs.io/en/stable/\n\n- ReportLab (PDF Generation): https://www.reportlab.com/docs/reportlab-userguide.pdf\n\n- HTTPLib (`httpx` - async HTTP client): https://www.python-httpx.org/\n\n- Requests (HTTP client): https://requests.readthedocs.io/en/latest/\n\n- GitHub Actions: https://docs.github.com/en/actions\n\n- Ruff (Linter & Formatter): https://docs.astral.sh/ruff/\n\n- Poetry (Dependency Management): https://python-poetry.org/docs/\n\n- Pytest (Testing Framework): https://docs.pytest.org/en/stable/\n\n- Standard Library `smtplib` (Email): https://docs.python.org/3/library/smtplib.html\n\n- Standard Library `email.mime`: https://docs.python.org/3/library/email.mime.html\n\n\n\n\n## Project-Specific Guidelines\n\n- Follow best practices for Python development.\n- Use standard conventions for Automated Data Analysis, AI-Powered Prediction, and Reporting System projects.\n- Implement secure coding practices and proper error handling.\n- Add appropriate documentation and comments.\n\n\n\n## Helpful Context\n\n- Consider the overall project architecture and design patterns.\n",
  "project_type": "Automated Data Analysis, AI-Powered Prediction, and Reporting System",
  "programming_language": "Python",
  "error": null,
  "stack_trace": null
}